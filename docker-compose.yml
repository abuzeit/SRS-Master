# =============================================================================
# SRS Master — Docker Compose Stack
# =============================================================================
#
# Complete deployment for the SCADA data aggregation system:
#
#   ┌─────────────────────────────────────────────────────────┐
#   │  kafka-1, kafka-2, kafka-3  — KRaft mode (no ZooKeeper) │
#   │  postgres                    — Historian database         │
#   │  producer                    — Level-2 SCADA node         │
#   │  consumer                    — Level-3 master aggregator  │
#   │  kafka-init                  — One-shot topic creation    │
#   └─────────────────────────────────────────────────────────┘
#
# Usage:
#   docker compose up -d          # Start all services
#   docker compose logs -f        # Stream all logs
#   docker compose down           # Stop and remove containers
#   docker compose down -v        # Stop and remove containers + volumes
#
# For production with TLS:
#   1. Run kafka/scripts/generate-certs.sh first
#   2. Uncomment the TLS volume mounts below
#   3. Set proper SASL passwords in .env
# =============================================================================

services:

  # ===========================================================================
  # Kafka Broker 1 (KRaft mode — combined controller + broker)
  # ===========================================================================
  kafka-1:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka-1
    container_name: srs-kafka-1
    ports:
      - "9092:9092" # External client access
    environment:
      # KRaft Configuration — No ZooKeeper required
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093"
      CLUSTER_ID: "SRS-SCADA-KAFKA-CLUSTER-01"

      # Listener Configuration
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093,INTERNAL://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-1:9092,INTERNAL://kafka-1:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"

      # Replication for fault tolerance
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 200

      # Performance tuning for industrial workload
      KAFKA_LOG_RETENTION_HOURS: 168 # 7 days default
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB segments
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_MESSAGE_MAX_BYTES: 1048576 # 1MB max message

      # KRaft storage
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      - scada-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 | grep -q 'ApiVersion'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ===========================================================================
  # Kafka Broker 2
  # ===========================================================================
  kafka-2:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka-2
    container_name: srs-kafka-2
    ports:
      - "9094:9092"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093"
      CLUSTER_ID: "SRS-SCADA-KAFKA-CLUSTER-01"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093,INTERNAL://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-2:9092,INTERNAL://kafka-2:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 200
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_MESSAGE_MAX_BYTES: 1048576
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    networks:
      - scada-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 | grep -q 'ApiVersion'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ===========================================================================
  # Kafka Broker 3
  # ===========================================================================
  kafka-3:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka-3
    container_name: srs-kafka-3
    ports:
      - "9095:9092"
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093"
      CLUSTER_ID: "SRS-SCADA-KAFKA-CLUSTER-01"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093,INTERNAL://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka-3:9092,INTERNAL://kafka-3:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 200
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_MESSAGE_MAX_BYTES: 1048576
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    networks:
      - scada-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 | grep -q 'ApiVersion'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ===========================================================================
  # Kafka Topic Initialization (one-shot container)
  # ===========================================================================
  kafka-init:
    image: confluentinc/cp-kafka:7.6.0
    container_name: srs-kafka-init
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    volumes:
      - ./kafka/scripts:/scripts:ro
    entrypoint: [ "/bin/bash", "-c" ]
    command:
      - |
        echo "Waiting for Kafka cluster to stabilize..."
        sleep 10

        echo "Creating topics..."
        KAFKA_BIN="" /scripts/create-topics.sh kafka-1:9092

        echo "Topic initialization complete."
    networks:
      - scada-network
    restart: "no"

  # ===========================================================================
  # PostgreSQL — Central Historian Database
  # ===========================================================================
  postgres:
    image: postgres:16-alpine
    hostname: postgres
    container_name: srs-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: scada_historian
      POSTGRES_USER: scada
      POSTGRES_PASSWORD: ${DB_PASSWORD:-scada-secret-change-me}
      # Performance tuning for historian workload
      POSTGRES_INITDB_ARGS: "--data-checksums"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
    networks:
      - scada-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U scada -d scada_historian" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=128MB"
      - "-c"
      - "effective_cache_size=512MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "random_page_cost=1.1"

  # ===========================================================================
  # Producer Local PostgreSQL — Outbox Database (one per node in production)
  # ===========================================================================
  producer-postgres:
    image: postgres:16-alpine
    hostname: producer-postgres
    container_name: srs-producer-postgres
    ports:
      - "5433:5432"
    environment:
      POSTGRES_DB: scada_outbox
      POSTGRES_USER: producer
      POSTGRES_PASSWORD: ${PRODUCER_DB_PASSWORD:-producer-db-secret}
      POSTGRES_INITDB_ARGS: "--data-checksums"
    volumes:
      - producer-postgres-data:/var/lib/postgresql/data
    networks:
      - scada-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U producer -d scada_outbox" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ===========================================================================
  # Producer — SCADA Level-2 Remote Node (Outbox Pattern)
  # ===========================================================================
  producer:
    build:
      context: .
      dockerfile: packages/producer/Dockerfile
    container_name: srs-producer
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      producer-postgres:
        condition: service_healthy
    environment:
      NODE_ID: 1
      PLANT_ID: PLANT01
      AREA_ID: AREA01
      KAFKA_BROKERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      KAFKA_CLIENT_ID: scada-producer-01
      PRODUCER_INTERVAL_MS: 1000
      # Outbox — Local PostgreSQL for transactional outbox
      PRODUCER_DATABASE_URL: postgresql://producer:${PRODUCER_DB_PASSWORD:-producer-db-secret}@producer-postgres:5432/scada_outbox?schema=public
      # Outbox Dispatcher tuning
      OUTBOX_BATCH_SIZE: 50
      OUTBOX_POLL_INTERVAL_MS: 500
      OUTBOX_MAX_RETRIES: 10
      OUTBOX_LOCK_TIMEOUT_SECONDS: 120
      OUTBOX_RECOVERY_INTERVAL_MS: 30000
      LOG_LEVEL: info
      # TLS/SASL — uncomment for production
      # KAFKA_SASL_USERNAME: producer-user
      # KAFKA_SASL_PASSWORD: ${PRODUCER_PASSWORD}
      # KAFKA_SSL_CA_PATH: /certs/ca-cert.pem
      # KAFKA_SSL_CERT_PATH: /certs/producer-cert.pem
      # KAFKA_SSL_KEY_PATH: /certs/producer-key.pem
      # volumes:
      #   - ./kafka/certs:/certs:ro
    networks:
      - scada-network
    restart: unless-stopped

  # ===========================================================================
  # Consumer — SCADA Level-3 Master Aggregation Station
  # ===========================================================================
  consumer:
    build:
      context: .
      dockerfile: packages/consumer/Dockerfile
    container_name: srs-consumer
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    environment:
      KAFKA_BROKERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      KAFKA_CLIENT_ID: master-aggregator
      KAFKA_CONSUMER_GROUP_ID: master-aggregator
      DATABASE_URL: postgresql://scada:${DB_PASSWORD:-scada-secret-change-me}@postgres:5432/scada_historian?schema=public
      LOG_LEVEL: info
      # TLS/SASL — uncomment for production
      # KAFKA_SASL_USERNAME: consumer-user
      # KAFKA_SASL_PASSWORD: ${CONSUMER_PASSWORD}
      # KAFKA_SSL_CA_PATH: /certs/ca-cert.pem
      # KAFKA_SSL_CERT_PATH: /certs/consumer-cert.pem
      # KAFKA_SSL_KEY_PATH: /certs/consumer-key.pem
      # volumes:
      #   - ./kafka/certs:/certs:ro
    networks:
      - scada-network
    restart: unless-stopped

# =============================================================================
# Named Volumes — Persistent data storage
# =============================================================================
volumes:
  kafka-1-data:
    name: srs-kafka-1-data
  kafka-2-data:
    name: srs-kafka-2-data
  kafka-3-data:
    name: srs-kafka-3-data
  postgres-data:
    name: srs-postgres-data
  producer-postgres-data:
    name: srs-producer-postgres-data

# =============================================================================
# Network — Isolated SCADA network
# =============================================================================
networks:
  scada-network:
    name: srs-scada-network
    driver: bridge
